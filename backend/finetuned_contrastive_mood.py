# -*- coding: utf-8 -*-
"""finetuned_contrastive_mood.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19z9KECUlIGtYnh75H6ibz_xw27bNbgBC
"""

# # Mood → Album Contrastive Fine-tuning (GPU-ready Colab Notebook)
#
# This notebook fine-tunes a dual-encoder for matching mood queries to albums.
# It performs:
# 1. Data loading and preprocessing
# 2. Stage 1 training with MultipleNegativesRankingLoss
# 3. Encode albums and mine hard negatives (genre-aware)
# 4. Stage 2 training with TripletLoss (hard negatives)
# 5. Evaluation (Recall@1/5/10) and FAISS index demo
#
# **Requirements:** provide a CSV with columns:
# `query,album_id,title,artist,genre,year_released,review`
#
# Tweak hyperparameters below as needed.

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # 1) upgrade pip
# pip install -U pip
# 
# # 2) install required packages (let pip pick compatible numpy)
# pip install -q sentence-transformers pandas scikit-learn tqdm faiss-cpu
# 
#

import os
import math
import random
from pathlib import Path
from tqdm.auto import tqdm

import numpy as np
import pandas as pd

import torch
from sklearn.model_selection import train_test_split

from sentence_transformers import SentenceTransformer, InputExample, losses, util
from torch.utils.data import DataLoader

import faiss  # faiss-cpu

# ---------------- CONFIG ----------------
# Change these paths if needed
DATA_CSV = "/content/reviews_with_moods.csv"  # set to your uploaded CSV path
OUTPUT_DIR = "/content/mood_album_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Model & training hyperparams
BASE_MODEL = "all-mpnet-base-v2"  # change to 'all-MiniLM-L6-v2' for faster runs
BATCH_SIZE = 32
EPOCHS_STAGE1 = 2
EPOCHS_STAGE2 = 2
LR = 2e-5
MAX_LEN_QUERY = 64
MAX_LEN_ALBUM = 256

# Hard negative mining params
TOP_K = 20  # candidates to retrieve for mining hard negatives
HARD_NEG_PER_QUERY = 1

# Device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)
print("Torch version:", torch.__version__)

# Load CSV
df = pd.read_csv(DATA_CSV)
print("Rows loaded:", len(df))
print(df.columns.tolist())

# Basic checks
required_columns = ["query","album_id","title","artist","genre","year_released","review"]
missing = [c for c in required_columns if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

# Build album_text
def build_album_text(row, max_review_chars=2000):
    title = str(row.get("title","")).strip()
    artist = str(row.get("artist","")).strip()
    genre = str(row.get("genre","")).strip()
    year = str(row.get("year_released","")).strip()
    review = str(row.get("review","")).strip()
    if len(review) > max_review_chars:
        review = review[:max_review_chars] + "..."
    pieces = []
    if title:
        pieces.append(f"{title}")
    if artist:
        pieces.append(f"by {artist}")
    meta = []
    if genre:
        meta.append(f"Genre: {genre}")
    if year and str(year).lower() not in ["nan","none",""]:
        meta.append(f"Released: {year}")
    album_text = ". ".join(pieces) + ". " + "; ".join(meta) + ". Review: " + review
    return album_text

df["album_text"] = df.apply(build_album_text, axis=1)
# optional: drop rows with empty album_text or empty query
df = df.dropna(subset=["query","album_text"])
df = df.reset_index(drop=True)
print("After build, rows:", len(df))
df.head(2)

# Ensure album-level split: no album_id in multiple splits
album_ids = df["album_id"].unique().tolist()
train_album_ids, temp_album_ids = train_test_split(album_ids, test_size=0.10, random_state=42)  # 90/10 album split
dev_album_ids, test_album_ids = train_test_split(temp_album_ids, test_size=0.5, random_state=42)  # split 10% -> 5/5

def assign_split(row):
    aid = row["album_id"]
    if aid in train_album_ids:
        return "train"
    elif aid in dev_album_ids:
        return "dev"
    elif aid in test_album_ids:
        return "test"
    else:
        return "train"  # fallback

df["split"] = df.apply(assign_split, axis=1)
print(df["split"].value_counts())
# For training we use rows where split == 'train'

train_df = df[df["split"] == "train"].reset_index(drop=True)
dev_df = df[df["split"] == "dev"].reset_index(drop=True)
test_df = df[df["split"] == "test"].reset_index(drop=True)

print("Train examples:", len(train_df))
print("Dev examples:", len(dev_df))
print("Test examples:", len(test_df))

# Build InputExample list for training
train_examples = [InputExample(texts=[row["query"], row["album_text"]]) for _, row in train_df.iterrows()]
dev_examples = [InputExample(texts=[row["query"], row["album_text"]]) for _, row in dev_df.iterrows()]

# small sanity: sample example
if len(train_examples) > 0:
    print("Sample training pair:")
    print(train_examples[0].texts)

model = SentenceTransformer(BASE_MODEL, device=DEVICE)
print("Model loaded:", BASE_MODEL)

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)
train_loss = losses.MultipleNegativesRankingLoss(model)

# Optional evaluator: use CrossEncoder for reranking eval or leave for offline recall calculation later.
# Train
model_save_path_stage1 = os.path.join(OUTPUT_DIR, "stage1")
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=EPOCHS_STAGE1,
    warmup_steps= max(100, int(len(train_dataloader) * EPOCHS_STAGE1 * 0.1)),
    output_path=model_save_path_stage1,
    show_progress_bar=True
)
print("Stage 1 model saved to", model_save_path_stage1)

# Build unique album list (one entry per album_id)
albums_df = df.drop_duplicates(subset=["album_id"]).reset_index(drop=True)
album_texts = albums_df["album_text"].tolist()
album_ids = albums_df["album_id"].tolist()

print("Unique albums:", len(album_texts))

# Encode album texts
batch_size_encode = 128
album_embeddings = model.encode(album_texts, batch_size=batch_size_encode, convert_to_numpy=True, show_progress_bar=True)
# Normalize vectors for cosine similarity with IndexFlatIP
faiss.normalize_L2(album_embeddings)

dim = album_embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(album_embeddings)
print("FAISS index built: num vectors =", index.ntotal)

# Map album_id -> index position in albums_df
albumid_to_idx = {aid: idx for idx, aid in enumerate(album_ids)}
# Build genre mapping for album idx
album_genre = {albumid_to_idx[row["album_id"]]: row.get("genre","") for _, row in albums_df.iterrows()}

# Prepare train queries and their ground truth album idx
train_queries = train_df["query"].tolist()
train_pos_album_ids = train_df["album_id"].tolist()
train_pos_idxs = [albumid_to_idx[aid] for aid in train_pos_album_ids]

# Encode queries
query_embeddings = model.encode(train_queries, batch_size=batch_size_encode, convert_to_numpy=True, show_progress_bar=True)
faiss.normalize_L2(query_embeddings)

# Search top-K
D, I = index.search(query_embeddings.astype('float32'), TOP_K)  # I shape: (n_queries, TOP_K)
print("I shape:", I.shape)

# For each query, pick hard negatives: prefer same-genre but not the positive
hard_negatives = []
for qi in range(len(train_queries)):
    pos_idx = train_pos_idxs[qi]
    pos_genre = album_genre.get(pos_idx, "")
    candidates = I[qi].tolist()
    # remove positive if present
    candidates = [c for c in candidates if c != pos_idx]
    hard = None
    # first try to find candidate with same genre
    for c in candidates:
        if album_genre.get(c,"") == pos_genre:
            hard = c
            break
    # fallback: pick first candidate
    if hard is None and len(candidates) > 0:
        hard = candidates[0]
    if hard is not None:
        hard_negatives.append(hard)
    else:
        hard_negatives.append(None)

# Count how many had hard negatives
print("Hard negatives found:", sum(1 for h in hard_negatives if h is not None), "out of", len(hard_negatives))

#building triplet dataset
triplet_examples = []
for i, row in train_df.iterrows():
    q = row["query"]
    pos_aid = row["album_id"]
    pos_idx = albumid_to_idx[pos_aid]
    hard_idx = hard_negatives[i]
    if hard_idx is None:
        continue
    pos_text = albums_df.loc[pos_idx, "album_text"]
    neg_text = albums_df.loc[hard_idx, "album_text"]
    triplet_examples.append(InputExample(texts=[q, pos_text, neg_text]))

print("Triplets created:", len(triplet_examples))

# Create DataLoader
triplet_loader = DataLoader(triplet_examples, shuffle=True, batch_size=32)

from sentence_transformers import losses as st_losses
import torch.nn.functional as F
import torch

# Reload model
model = SentenceTransformer(model_save_path_stage1, device=DEVICE)

# Define a simple cosine distance function for TripletLoss
def cosine_distance_func(vec1, vec2):
    # Ensure the result is a scalar by using .squeeze()
    return 1 - F.cosine_similarity(vec1, vec2).squeeze()

# Use TripletLoss, as we have explicit triplets (query, positive, negative)
# TripletLoss expects a distance function that compares two embeddings (vec1, vec2).
# It also uses a triplet_margin.
triplet_loss = st_losses.TripletLoss(
    model=model,
    distance_metric=cosine_distance_func,
    triplet_margin=0.2  # TripletLoss typically uses a margin
)

model_save_path_stage2 = os.path.join(OUTPUT_DIR, "stage2")

model.fit(
    train_objectives=[(triplet_loader, triplet_loss)],
    epochs=EPOCHS_STAGE2,
    warmup_steps=max(50, int(len(triplet_loader) * EPOCHS_STAGE2 * 0.1)),
    output_path=model_save_path_stage2,
    show_progress_bar=True
)

print("Stage 2 model saved to", model_save_path_stage2)

#evaluate with test set
# Build test album list and index
test_albums_df = df[df["split"] == "test"].drop_duplicates(subset=["album_id"]).reset_index(drop=True)
test_album_texts = test_albums_df["album_text"].tolist()
test_album_ids = test_albums_df["album_id"].tolist()
print("Test unique albums:", len(test_album_texts))

test_album_embeds = model.encode(test_album_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=True)
faiss.normalize_L2(test_album_embeds)
dim = test_album_embeds.shape[1]
test_index = faiss.IndexFlatIP(dim)
test_index.add(test_album_embeds)

# Build mapping album_id -> index
test_albumid_to_idx = {aid: idx for idx, aid in enumerate(test_album_ids)}

# Evaluate
test_queries = df[df["split"] == "test"]["query"].tolist()
test_pos_album_ids = df[df["split"] == "test"]["album_id"].tolist()
test_query_embeds = model.encode(test_queries, convert_to_numpy=True, batch_size=128, show_progress_bar=True)
faiss.normalize_L2(test_query_embeds)
K = 10
D, I = test_index.search(test_query_embeds.astype('float32'), K)

def compute_recall_at_k(I, pos_ids, idx_map, k):
    hits = 0
    for i, pos_aid in enumerate(pos_ids):
        if pos_aid not in idx_map:
            continue
        pos_idx = idx_map[pos_aid]
        if pos_idx in I[i,:k]:
            hits += 1
    return hits / len(pos_ids)

r1 = compute_recall_at_k(I, test_pos_album_ids, test_albumid_to_idx, 1)
r5 = compute_recall_at_k(I, test_pos_album_ids, test_albumid_to_idx, 5)
r10 = compute_recall_at_k(I, test_pos_album_ids, test_albumid_to_idx, 10)
print(f"Recall@1: {r1:.4f}, Recall@5: {r5:.4f}, Recall@10: {r10:.4f}")

# Encode all albums with final model
all_albums_df = df.drop_duplicates(subset=["album_id"]).reset_index(drop=True)
all_album_texts = all_albums_df["album_text"].tolist()
all_album_ids = all_albums_df["album_id"].tolist()

all_album_embeds = model.encode(all_album_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=True)
faiss.normalize_L2(all_album_embeds)

dim = all_album_embeds.shape[1]
full_index = faiss.IndexFlatIP(dim)
full_index.add(all_album_embeds)
print("Full index size:", full_index.ntotal)

# Save index to disk (optional)
faiss.write_index(full_index, os.path.join(OUTPUT_DIR, "full_albums.index"))
# Save album metadata mapping
all_albums_df.to_csv(os.path.join(OUTPUT_DIR, "albums_metadata.csv"), index=False)

# Demo function
def retrieve_mood(query, top_k=10):
    q_emb = model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)
    D, I = full_index.search(q_emb.astype('float32'), top_k)
    results = []
    for idx in I[0]:
        meta = all_albums_df.loc[idx]
        results.append({
            "album_id": meta["album_id"],
            "title": meta["title"],
            "artist": meta["artist"],
            "genre": meta["genre"],
            "album_text_snippet": meta["album_text"][:400]
        })
    return results

# Try some demo queries
for q in ["dreamy late-night", "urgent kinetic dance", "nostalgic folk"]:
    print("=== QUERY:", q)
    res = retrieve_mood(q, top_k=5)
    for r in res:
        print(r["title"], "—", r["artist"], "|", r["genre"])
    print()

#uploading model to huggingface
# !pip install huggingface_hub

# from huggingface_hub import login
# login()

# from transformers import AutoModelForCausalLM, AutoTokenizer
# from huggingface_hub import HfApi, Repository

# # push to hub (requires hf token)
# !pip install huggingface_hub
# from huggingface_hub import upload_folder
# upload_folder(folder_path="/content/mood_album_model", path_in_repo="", repo_id="mmarkusmalone/album_moods_embedding", token="")